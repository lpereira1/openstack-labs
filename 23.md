---
date: "2016-10-09"
draft: false
weight: 230
title: "Lab 23 - Install Networking on a freshly booted cloud"
---

### Install Networking on a freshly booted cloud - &#x2B50;REQUIRED&#x2B50;

### WEDNESDAY

### Lab Objective

The objective of this lab is to teach what the networking aspects of a freshly deployed OpenStack cloud look like. In this lab, we'll remove any preconfig Alta3 did on your behalf, and then put it back into place.

### Procedure

0. Close any old terminals you might have open on your remote desktop environment.

0. We can watch the results of the command line in Horizon. This is very instructional, so within your remote dekstop, sign into Horizon as **:default:admin:alta3:**

0. Click on **Project > Network > Network Topology**.

0. Study what is being presented. You can left click on the icons, and little pop-ups will show up explaining those icons in detail. The **provider-net**, **demo-net**, and **demorouter** were set up (by Alta3) for the admin project before you entered class. If you need help figuring out what is being presented to you, check the screenshot below.

	![Virtual Networking](https://alta3.com/labs/images/alta3_lab_horizon_network_topology.png)

0. Leave the Horizon session open so you can see it. Meanwhile, launch a new terminal session on your remote desktop, then connect to controller.

    `student@beachhead:/$` `ssh controller`
    
0. Source the admin.rc file

    `student@controller:~$` `source admin.rc`

0. All instances must be deleted, so list all running instances and delete them all.

    `student@controller (admin) :~$` `openstack server list --all-projects`  

   ```
   +--------------------------------------+--------+--------+--------------------------+
   | ID                                   | Name   | Status | Networks                 |
   +--------------------------------------+--------+--------+--------------------------+
   | 8bd0d720-4053-4a36-a243-4b8edca76c3a | vt1    | ACTIVE | provider-net=172.16.2.57 |
   +--------------------------------------+--------+--------+--------------------------+
   ```
   
0. If you made some additional instances, you might see more. If you already deleted vt1, you'll see less. In any event, issue the following command against each virtual machine.

    `student@controller (admin) :~$` `openstack server delete <UUID_listed_in_ID_column>`

0. Ask again for a display of all the running instances in the cloud. Nothing should be returned.

    `student@controller (admin) :~$` `openstack server list --all-projects`  

   ```
   +--------------------------------------+--------+--------+--------------------------+
   | ID                                   | Name   | Status | Networks                 |
   +--------------------------------------+--------+--------+--------------------------+
   +--------------------------------------+--------+--------+--------------------------+
   ```

0. Great! Now we can remove the networking 

0. Discover all routers that must be deleted  
    a. `neutron router-list --max-width 60`  - List all the routers  
     b. `neutron subnet-list` - List all subnets  


4. Delete all network configuration, top down, interfaces first, then the router, then subnets, then networks.

    a. `neutron router-interface-delete demorouter demo-net_subnet`  - Remove 

    b. `neutron router-delete demorouter provider-subnet`    

    c. `neutron subnet-delete demo-net_subnet`

    d. `neutron net-delete demo-net`

    e. `neutron subnet-delete provider-net`

    f. `neutron net-delete provider-net`
   > OK, you have now removed all the demonstration configuration. Let's put it all back, step by step. 

	   ![Lab Environment](https://alta3.com/labs/images/alta3_lab_delete_network_order.png)


5. Neutron is running on your controller student network has TWO NICs, connected to two subnets, both connecting to the internet
   Admin network: 172.16.1.0/24
   Provider network: 172.16.2.0/24

   > Which NIC is assigned to the admin network, and which is assigned to the provider network

6. list the NICs

   `ip addr`

  ```
  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host
        valid_lft forever preferred_lft forever
  2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc pfifo_fast state UP group default qlen 1000
      link/ether fa:16:3e:2f:61:74 brd ff:ff:ff:ff:ff:ff
      inet 172.16.1.5/24 brd 172.16.1.255 scope global ens3  
         valid_lft forever preferred_lft forever
      inet6 fe80::f816:3eff:fe2f:6174/64 scope link
         valid_lft forever preferred_lft forever
  3: ens4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 
      link/ether fa:16:3e:be:52:55 brd ff:ff:ff:ff:ff:ff
      inet6 fe80::f816:3eff:febe:5255/64 scope link   
         valid_lft forever preferred_lft forever
  ```

7. So ens3 is assiged to the admin network by virtue of its ip address 172.16.1.5/24 which is the physical administration network. So is "ens4" the Provider NIC? Let's find out for sure. We will follow a two-step process. So first you must discover which mechanism_driver is specified in ml2_conf.ini. The egrep filter skips the comments, listing only the configuration:

      `cat  /etc/neutron/plugins/ml2/ml2_conf.ini | egrep -v "(^#.*|^$)"`

```
[DEFAULT]
[ml2]
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = linuxbridge,l2population
extension_drivers = port_security
[ml2_type_flat]
flat_networks = provider
[ml2_type_geneve]
[ml2_type_gre]
[ml2_type_vlan]
[ml2_type_vxlan]
vni_ranges = 1:1000
[securitygroup]
enable_ipset = true
```

  > Looks like the mechanism_drivers = linuxbridge,l2population  
8. So now lets look for the NIC that is associated with the linuxbridge

  `cat  /etc/neutron/plugins/ml2/linuxbridge_agent.ini | egrep -v "(^#.*|^$)"`

```
[DEFAULT]
[agent]
[linux_bridge]
physical_interface_mappings = provider:ens4
[securitygroup]
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
enable_security_group = true
[vxlan]
enable_vxlan = true
local_ip = 172.16.1.5
l2_population = true
```

    > Ahhhah! We see `physical_interface_mappings = provider:ens4`  
    > This means the physical network interface is called "provider" and is mapped to "ens4"  
    > Some cloud operators call this "external", "public", or "fabric", so your network may use a different name here.


9. Now associate the physical interface to the provider network, with a network name "provider-net"  
  `neutron net-create --shared --provider:physical_network provider --provider:network_type flat provider-net`

10. Define a new subnet, called provider-subnet" and assign it to "provider-net"   
   `neutron subnet-create --name provider-subnet  --allocation-pool start=172.16.2.50,end=172.16.2.250 --dns-nameserver 10.3.200.1  --gateway 172.16.2.1 provider-net 172.16.2.0/24`

11. Create a new subnet, sperate from the provider net. Call it "demo-net"  
  `neutron net-create demo-net`

12. Create a new subnet called demo-net_subnet and assign demo-net_subnet to demo-net
  `neutron subnet-create --name demo-net_subnet --dns-nameserver 10.3.200.1 --gateway 192.168.30.1 demo-net 192.168.30.0/24`  

13.  Make the provider-net network the service provider.  
  `neutron net-update provider-net --router:external`

14.  Creat a virtual router called "demorouter"
  `neutron router-create demorouter`

15. Connect demorouter to demo-net_subnet  
  `neutron router-interface-add demorouter demo-net_subnet`


16.  Connect demorouter to povider-net.  
  `neutron router-gateway-set demorouter provider-net`
