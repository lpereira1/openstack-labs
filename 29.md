---
date: "2016-10-09"
draft: false
weight: 290
title: "Lab 29 - Creating Block Storage Volumes with Cinder at the CLI"
---
[Click here to find out more about Alta3 Research's Openstack Training](https://alta3.com/courses/openstack)

### THURSDAY- &#x2B50;REQUIRED&#x2B50;

### Lab Objective 

The objective of this lab is to create a block storage volume with Cinder at the CLI, attach it to a VM instance, create some data on that volume, detach that volume, attach it to a new VM, and then confirm that the data moved with the cinder volume.

### Procedure

0. We don't need any old terminal windows open, so close them all, and then open a new one. Once you have a new terminal window open, SSH to controller. 

    `student@beachhead:/$` `ssh controller`

0. The CLI prompt should now read as follows:

    `student@controller:~$`

0. Source the chester.rc permissions file

    `student@controller:~$` `source chester.rc`

0. Create a new volume called **NASferatu** a **1 Gb** in size/

    `student@controller (chester) :~$` `openstack volume create --size 1 NASferatu`

    ![Create 1 Gb Volume](https://alta3.com/labs/images/alta3_lab_volume_create.png)

0. Based on the output, answer the following questions:

    - **Q1: Is this a bootable volume?**
      - A1: No. The value of bootable is false.
    - **Q2: Is this volume encrypted?**
      - Q2: No. The value of encrypted is false.
    - **Q3: What is the status?**
      - Q3: The status is listed as creating

0. Issue the following command to retrieve a list of cinder volumes available to the current users chestercopperpot:

    `student@controller (chester) :~$` `openstack volume list`
    
    ```
    +--------------------------------------+--------------+-----------+------+------------------------------+
    | ID                                   | Display Name | Status    | Size | Attached to                  |
    +--------------------------------------+--------------+-----------+------+------------------------------+
    | 857334ec-92ce-43dc-83b5-b801b77337c6 | NASferatu    | available |    1 |                              |
    | d37ab4c8-786e-4915-a94c-0c5ff5e3c20c | cargo-bay    | in-use    |    1 | Attached to vt1 on /dev/vdb  |
    +--------------------------------------+--------------+-----------+------+------------------------------+
    ```
    >
    You'll need to use the UUID for NASferatu in the next few steps. 
    
0. Based on the output from the previous command, *openstack volume list*, answer the following questions.
	
    - **Q1: What is the ID of NASferatu?**
      - A1: NASferatu is identified by an UUID revealed by the openstack volume list command
    - **Q2: Has the status changed?**
      - Q2: Yes. The status is now listed as available.

0. Issue the following command to retrive a list of the instances currently in operation. Note the ID of vt2.
	
    `student@controller (chester) :~$` `openstack server list`
    
    ```
    +--------------------------------------+-----------+--------+-----------------------------+
    | ID                                   | Name      | Status | Networks                    |
    +--------------------------------------+-----------+--------+-----------------------------+
    | 835a6280-7f4f-4d9a-9705-5c9b8e64a317 | vt2       | ACTIVE | vault-tek-network=10.10.0.3 |
    +--------------------------------------+-----------+--------+-----------------------------+
    ```
    >
    You'll need to use the UUID for vt2 that you just retrieved in the next few steps. 
	
0. **IMPORTANT:** You'll need to customize this step. Issue the following command to attach the new volume (NASferatu) to a VM instance (vt2)
	
    `student@controller (chester) :~$` `openstack server add volume <UUID_OF_INSTANCE_vt2> <UUID_OF_NASferatu>`
	
0. Confirm that NASferatu has been Attached to vt2
	
    `student@controller (chester) :~$` `openstack volume list`
	
0. Issue the following command to detach the volume from vt2

    `ender@controller chester >` `openstack server remove volume <replace_with_ID_of_instance_vt2> <replace_with_ID_of_volume_NASferatu>`
	
0. Check on the status of the volume (NASferatu)
	
    `ender@controller chester >` `openstack volume list`

    - **Q1: What is the status of NASferatu?**
      - A1: It has changed from in-use, to available.

0. Now that the volume is detached, delete it.
	
    `ender@controller chester >` `openstack volume delete <replace_with_ID_of_volume_NASferatu>`

0. Confirm that the volume has been deleted.

    `ender@controller chester >`  `openstack volume list`
	
    - **Q1: What is the status of NASferatu?**
      - A1: NASferatu has been deleted and is no longer available.

0. In the next section, you will SSH into vt2. You should have started it in Horizon, but take a moment and make sure vt2 is powered on (and not shutoff). You can see which VMs are powered up, or down, by using the **nova list** command. If needed, vt2 can be started with the **nova start vt2** command.

1.  Discover the IP address of vt2, later referenced as `<IP_ADDRESS_of_vt2>` 
 
    `nova show vt2 | grep network`

    ```
    | vault-tek-network network            | 10.10.0.4    
    ```

2. Discover the router ID which will be needed to discover network namespace ID prefixed with "qrouter-"

    `neutron router-list | cut -c -59`

    ```
    +--------------------------------------+------------------+
    | id                                   | name             |
    +--------------------------------------+------------------+
    | 5fddef2e-5cf4-4bb9-8617-57c2b9be8489 | vault-tek-router |
    +--------------------------------------+------------------+
    ```

3. SSH to neutron

    `ssh root@neutron`

    ```
    [root@neutron ~]# 
    ```

4. List the namespaces and find the "qrouter-" entry whose sufix matches the vault-tek-router's ID

    `ip netns list`

    ```
    qdhcp-20ca30a2-e3fb-4e62-bd78-08dc471e93ed
    qdhcp-d5d84797-f546-4c41-8f63-7c44954fcb8b
    qdhcp-854e1cb3-9234-43ce-b467-47d5bfdf3539
    qrouter-fe2f3cd2-9099-41d8-b231-c567b5cb4802
    qrouter-5fddef2e-5cf4-4bb9-8617-57c2b9be8489
    ```

5. SSH into vt2 via the namespace you discovered in previous step

    `ip netns exec <NAMESPACE> ssh cirros@<IP_ADDRESS_of_vt2>`

    ```
    [root@neutron ~]# ip netns exec qrouter-5fddef2e-5cf4-4bb9-8617-57c2b9be8489 ssh cirros@10.10.0.4
    cirros@10.10.0.4's password:
    ```
    > If you receive a 'no route to host' error, it is most likely because vt2 is powered off. To resolve, hop into Horizon as chestercopperpot, click on **Project > Compute > Instances** and then select **Start Instance** beside vt2.
    
6. Here is the password

    `cubswin:)`

7. Install a filesystem (assuming the drive is /dev/vdb)

    `sudo mkfs.ext4 /dev/vdb`

8. Create a directory to serve as the mount point

    `sudo mkdir -p /mnt/cargobay`

9. Mount the new directory

    `sudo mount /dev/vdb /mnt/cargobay`

10. Who are you?

    `whoami`

11. change the owner of cargo bay so you can easily add files as user = whoami

    `sudo chown -R cirros  /mnt/cargobay/`

12. Change directory into the newly mounted volume

    `cd /mnt/cargobay`

13. Add a test message to /mnt/cargobay (your present location) with the following command.

    `echo "This is a test message written to my cargo bay volume!" > test.txt`

14. Confirm that your file is there
  
    `ls -l`

15. Unmount the volume in preperation for moving it to a new instance. **(VERY IMPORANT)**

    `cd`     <-- be sure to issue this command first, otherwise you cannot unmount (can't saw off a limb you're standing on!)

    `sudo umount /mnt/cargobay`

16. Exit your SSH session to vt2

    `exit`

17. exit back to controller

    `exit`

18. Discover the vault-tek-network ID

    `neutron net-list | grep vault`

    ```
    |       ID of vault-tek-network        | Name              | subnet-ID
    ------------------------------------------------------------------------------------------------------------------
    | 854e1cb3-9234-43ce-b467-47d5bfdf3539 | vault-tek-network | 36ac8b4a-296f-4992-8248-dfccf2125da0 10.10.0.0/24    |
    ```

19. Start a new instance (vt3... whatever you want). Use the same security-group (http-ssh) and network as vt2. Here is a sample if you want to try booting the new instance from the command line. Be sure the edit the net-id below because this one is **just an example**.

    `nova boot --flavor m1.tiny --image cirros --nic net-id=<ID of vault-tek-network in the previous step> --security-groups http-ssh   vt3`

20. Want to do a little practice?  Delete the above VM and start it again on your own, both from the dashboard and also from the command line. WARNING, ONLY two machines will run and one time!!!

21. Discover instance IDs

    `nova list`

    ```
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    | ID                                   | Name | Status  | Task State | Power State | Networks                    |
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    | fd70c522-28da-4607-9610-af766440ac1a | vt2  | SHUTOFF | -          | Shutdown    | vault-tek-network=10.10.0.4 |
    | 1accbf8d-0d0c-43be-b314-4f8d45672929 | vt3  | ACTIVE  | -          | Running     | vault-tek-network=10.10.0.5 |
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    ```

22. Discover cinder volumes

    `cinder list`

    ```
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    |                  ID                  | Status | Display Name | Size | Volume Type | Bootable |             Attached to              |
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    | b9f00354-0df1-4d5f-bc23-6423478cf334 | in-use |  cargo-bay   |  1   |    iscsi    |  false   | fd70c522-28da-4607-9610-af766440ac1a |
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    ```

23. Here is a nice way to discover where this volume is connected in your instance! 

    `cinder show cargo-bay | grep /dev/`  

    > Most likely the volume will be connected to: `/dev/vdb` 


24. Detach cargo-bay

    `nova volume-detach <vt2-ID> <cargo-bay_ID>`

25. Confirm detachment  (Wait until status changes from "detaching" to "Available"

    `cinder list`

    ```
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    |                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    | b9f00354-0df1-4d5f-bc23-6423478cf334 | detaching |  cargo-bay   |  1   |    iscsi    |  false   | fd70c522-28da-4607-9610-af766440ac1a |
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    ```

25. The above status says 'detaching'; status needs to read 'available' before you can continue to attach the volume cargo-bay to the new instance vt3.

    `nova volume-attach <vt3-ID> <cargo-bay-ID> auto`

26. SSH into vt3 (reference steps 1 to 6 for help) 

27. Mount cargo bay (reference steps 8 to 12 for help) <-- IMPORTANT !!!DO NOT REPERFORM STEP 7 OR YOU WILL DELETE YOUR TEXT FILE!!!

28. Look at the new volume on vt3 and confirm it contains the test file that you created.

